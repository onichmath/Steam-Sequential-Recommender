{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d63a9cc-5c1b-4847-b549-48125c57b789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthewo/Documents/College/CSE258/ass2/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import implicit # For BPR\n",
    "from scipy.sparse import coo_matrix, csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345054f-2407-469e-8571-34986573e130",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ac105b-4d1f-47cd-8e00-bf7aa2ffde5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 715613\n",
    "val_len = 92112\n",
    "test_len = 92112\n",
    "total_len = train_len + val_len + test_len\n",
    "assert total_len == 899837\n",
    "num_users = 113_979\n",
    "num_items = 8_609\n",
    "k_vals = [3, 5, 10, 25, 50]\n",
    "num_items = 64\n",
    "SEED = 42\n",
    "\n",
    "def build_results_df(model_name, k_vals, hits_at_k, precision_at_k):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the results of the model, given lists of k_vals, hits_at_k, and precision_at_k\n",
    "    \"\"\"\n",
    "    return pl.DataFrame(\n",
    "        {\n",
    "            \"model\": [model_name] * len(k_vals),\n",
    "            \"k\": k_vals,\n",
    "            \"hits_at_k\": hits_at_k,\n",
    "            \"precision_at_k\": precision_at_k\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d0e66-85cb-42d8-86d4-4a88e2ddbdbf",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a051eb4c-ec56-4e9e-8838-15dcdf839e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_data():\n",
    "    \"\"\"\n",
    "    Returns the full dataset sorted by user_id\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"hours\", \"products\", \"found_funny\", \"hours\", \"text\"]\n",
    "    df = pl.scan_parquet(\"data/reviews.parquet\")\n",
    "    print(df.collect())\n",
    "\n",
    "def load_train_val_test_data(): \n",
    "    \"\"\"\n",
    "    Returns datasets sorted by user_id with idx of their initial position in the dataset\n",
    "    - X_train: All interactions except the last two per user\n",
    "    - y_valid: Second to last interaction per user\n",
    "    - y_test: Last interaction per user\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"hours\", \"products\", \"found_funny\", \"hours\", \"text\"]\n",
    "    full = pl.scan_parquet(\"data/sorted_reviews.parquet\").drop(exclude_cols).with_row_index(\"idx\")\n",
    "    users_with_less_than_2_interactions = full.group_by(\"mapped_user_id\").agg(pl.count(\"mapped_product_id\").alias(\"interaction_count\"))\n",
    "    users_with_less_than_2_interactions = users_with_less_than_2_interactions.filter(pl.col(\"interaction_count\") < 5)\n",
    "\n",
    "    y = full.group_by(\"mapped_user_id\").tail(2)\n",
    "    X_train = full.join(y, on=\"idx\", how=\"anti\") # Up to two interactions from last per user\n",
    "\n",
    "    y_valid = y.group_by(\"mapped_user_id\").head(1) # 1 interaction from last per user\n",
    "    y_test = y.group_by(\"mapped_user_id\").tail(1) # Last interaction per user\n",
    "\n",
    "    assert total_len == len(full.collect())\n",
    "    assert train_len == len(X_train.collect())\n",
    "    assert val_len == len(y_valid.collect())\n",
    "    assert test_len == len(y_test.collect())\n",
    "\n",
    "    return X_train, y_valid, y_test\n",
    "\n",
    "def format_train_eval_data(df, cols_to_keep=[\"idx\", \"user_id\", \"product_id\"]): \n",
    "    \"\"\"\n",
    "    Formats the data to have the columns: idx, user_id, product_id\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"review_date\", \"mapped_user_id\", \"mapped_product_id\"]\n",
    "    return (\n",
    "        df\n",
    "        .with_columns(\n",
    "            pl.col(\"mapped_user_id\").alias(\"user_id\"),\n",
    "            pl.col(\"mapped_product_id\").alias(\"product_id\"),\n",
    "        )\n",
    "        .drop(exclude_cols)\n",
    "        .select(cols_to_keep)\n",
    "    )\n",
    "\n",
    "def get_clean_train_valid_eval(): \n",
    "    \"\"\" \n",
    "    Returns the datasets formatted for training and evaluation\n",
    "    - X_train: All interactions except the last two per user\n",
    "    - y_valid: Second to last interaction per user\n",
    "    - y_test: Last interaction per user\n",
    "    \"\"\"\n",
    "    X_train, y_valid, y_test = load_train_val_test_data()\n",
    "    X_train = format_train_eval_data(X_train)\n",
    "    y_valid = format_train_eval_data(y_valid)\n",
    "    y_test = format_train_eval_data(y_test)\n",
    "    assert total_len == len(X_train.collect()) + len(y_valid.collect()) + len(y_test.collect())\n",
    "    return X_train, y_valid, y_test\n",
    "\n",
    "def join_x_y(X, y):\n",
    "    \"\"\"\n",
    "    Joins the training and validation datasets on each user \n",
    "    - X_test: All interactions except the last one per user\n",
    "    \"\"\"\n",
    "    X_join = pl.concat([X, y], how=\"vertical\").sort(\"idx\")\n",
    "    assert len(X_join.collect()) == len(X.collect()) + len(y.collect())\n",
    "    return X_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f92d4-43ae-4127-a268-5775802331a7",
   "metadata": {},
   "source": [
    "## Baseline Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503f8fe-fe31-4933-ab44-9c87352e92cd",
   "metadata": {},
   "source": [
    "### Experiment 1: Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d57e3a6-a358-403a-851d-9072b421e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_popular_items(X, k=50): \n",
    "    \"\"\"\n",
    "    Returns the k most popular items in X\n",
    "    \"\"\"\n",
    "    popular_items = (\n",
    "        X\n",
    "        .group_by(\"product_id\")\n",
    "        .agg(pl.count(\"product_id\").alias(\"count\"))\n",
    "        .sort(\"count\", descending=True)\n",
    "        .head(k)\n",
    "    )\n",
    "    return popular_items\n",
    "    \n",
    "def predict_based_on_item_popularity(X_train, y_valid, y_test, k=50): \n",
    "    \"\"\"\n",
    "    Predicts the most popular items for each user\n",
    "    \"\"\"\n",
    "    top_k_items = get_k_most_popular_items(\n",
    "        join_x_y(X_train, y_valid),\n",
    "        k)\n",
    "    return y_test.join(top_k_items, on=\"product_id\")\n",
    "\n",
    "def popularity_baseline(k_vals = k_vals):\n",
    "    X_train, y_valid, y_test = get_clean_train_valid_eval()\n",
    "    \n",
    "    hits_at_k = []\n",
    "    precision_at_k = []\n",
    "\n",
    "    for k in k_vals:\n",
    "        y_pred = predict_based_on_item_popularity(X_train, y_valid, y_test, k = k)\n",
    "        hits = len(y_pred.join(y_test, on=[\"user_id\", \"product_id\"]).collect())\n",
    "\n",
    "        num_predictions = len(y_pred.collect())\n",
    "        number_of_users = len(y_test.collect()) \n",
    "\n",
    "        hits_at_k.append(hits / number_of_users)\n",
    "\n",
    "        # Number of hits divided by number of predictions\n",
    "        precision_at_k.append(hits / k / number_of_users)\n",
    "\n",
    "    results = build_results_df(\"popularity\", k_vals, hits_at_k, precision_at_k)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c73ed84b-5a1b-4551-961c-254725879390",
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_baseline().write_csv(\"results/popularity_baseline.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89146ddb-b548-4ea1-90b5-2b2f12800ae2",
   "metadata": {},
   "source": [
    "### Experiment 2: Markov Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8bb0b90-03f9-4b26-afad-508226af3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix_from_markov_chain(X):\n",
    "    \"\"\"\n",
    "    Builds a transition matrix from transition counts\n",
    "    OLD Method, stopped using it since it cannot be Lazy\n",
    "    \"\"\"\n",
    "    return (\n",
    "            X.collect()\n",
    "            .pivot(\n",
    "                on=\"prev_product_id\",\n",
    "                index=\"product_id\",\n",
    "                values=\"transitions\",\n",
    "                aggregate_function=\"sum\"\n",
    "            )\n",
    "            .fill_null(0)\n",
    "            .rename({\"product_id\": \"next_product_id\"})\n",
    "            .with_columns(pl.all().exclude(\"next_product_id\") / pl.all().sum())\n",
    "            .lazy()\n",
    "        )\n",
    "\n",
    "def get_matrix_shifted_by_3(X):\n",
    "    \"\"\"\n",
    "    Returns matrix shifted by 3\n",
    "    \"\"\"\n",
    "    return (\n",
    "        X.with_columns([\n",
    "            pl.col(\"product_id\").shift(1).alias(\"prev_1_product_id\"),\n",
    "            pl.col(\"user_id\").shift(1).alias(\"prev_1_user_id\"),\n",
    "            pl.col(\"product_id\").shift(2).alias(\"prev_2_product_id\"),\n",
    "            pl.col(\"user_id\").shift(2).alias(\"prev_2_user_id\"),\n",
    "            pl.col(\"product_id\").shift(3).alias(\"prev_3_product_id\"),\n",
    "            pl.col(\"user_id\").shift(3).alias(\"prev_3_user_id\"),\n",
    "        ])\n",
    "        .filter(pl.col(\"prev_3_product_id\").is_not_null())\n",
    "        .filter(pl.col(\"user_id\") == pl.col(\"prev_3_user_id\"))\n",
    "    )\n",
    "\n",
    "def get_previous_n_transitions(X, n=1):\n",
    "    \"\"\"\n",
    "    Returns the previous n transitions for each user weighted by how far back they are\n",
    "    \"\"\"\n",
    "    prev_column = f\"prev_{n}_product_id\"\n",
    "    return (\n",
    "        X\n",
    "        .group_by([prev_column, \"product_id\"])\n",
    "        .len().rename({\"len\": \"transitions\"})\n",
    "        .with_columns([pl.col(\"transitions\") / n])\n",
    "        .rename({prev_column: \"prev_product_id\"})\n",
    "    )\n",
    "\n",
    "def get_3rd_order_markov_chain(X):\n",
    "    \"\"\"\n",
    "    Returns the transition probabilities for a 3rd order Markov chain\n",
    "    \"\"\"\n",
    "    X_shifted = get_matrix_shifted_by_3(X)\n",
    "    \n",
    "    n_1_transition_counts = get_previous_n_transitions(X_shifted, n=1)\n",
    "    n_2_transition_counts = get_previous_n_transitions(X_shifted, n=2)\n",
    "    n_3_transition_counts = get_previous_n_transitions(X_shifted, n=3)\n",
    "\n",
    "    transition_counts = pl.concat([n_1_transition_counts, n_2_transition_counts, n_3_transition_counts])\n",
    "    \n",
    "    transition_probs = (\n",
    "        transition_counts\n",
    "        .with_columns(pl.col(\"transitions\") / pl.col(\"transitions\").sum().over(\"prev_product_id\"))\n",
    "        .rename({\n",
    "            \"transitions\": \"probability\",\n",
    "            \"product_id\": \"next_product_id\",\n",
    "            \"prev_product_id\": \"product_id\",\n",
    "            })\n",
    "    )\n",
    "    \n",
    "    return transition_probs\n",
    "\n",
    "def predict_based_on_markov_chain(y, transition_matrix, k=50):\n",
    "    \"\"\"\n",
    "    Predicts the next k items based on the transition matrix\n",
    "    \"\"\"\n",
    "    y_pred =  (\n",
    "        y\n",
    "        .drop(\"idx\")\n",
    "        .join(transition_matrix, on=\"product_id\", how=\"left\")\n",
    "        .group_by(\"user_id\")\n",
    "        .agg(\n",
    "            [\n",
    "                pl.struct([\"next_product_id\", \"probability\"])\n",
    "                .sort_by(\"probability\", descending=True)\n",
    "                .slice(0, k)\n",
    "                .alias(\"top_predictions\")\n",
    "            ]\n",
    "        )\n",
    "        .explode(\"top_predictions\")\n",
    "        .select(\n",
    "            \"user_id\",\n",
    "            pl.col(\"top_predictions\").struct.field(\"next_product_id\").alias(\"product_id\"),\n",
    "            pl.col(\"top_predictions\").struct.field(\"probability\").alias(\"probability\"),\n",
    "        )\n",
    "    )\n",
    "    return y_pred\n",
    "\n",
    "def markov_chain_experiment(k_vals = k_vals):\n",
    "    X_train, y_valid, y_test = get_clean_train_valid_eval()\n",
    "    X_test = join_x_y(X_train, y_valid)\n",
    "\n",
    "    transition_matrix = get_3rd_order_markov_chain(X_test)\n",
    "    hits_at_k = []\n",
    "    precision_at_k = []\n",
    "\n",
    "    for k in k_vals:\n",
    "        y_pred = predict_based_on_markov_chain(y_valid, transition_matrix, k = k)\n",
    "        hits = len(y_pred.join(y_test, on=[\"user_id\", \"product_id\"]).collect())\n",
    "\n",
    "        num_predictions = len(y_pred.collect())\n",
    "        number_of_users = len(y_test.collect()) \n",
    "\n",
    "        hits_at_k.append(hits / number_of_users)\n",
    "        precision_at_k.append(hits / k / number_of_users)\n",
    "\n",
    "    results = build_results_df(\"Markov Chain\", k_vals=k_vals, hits_at_k=hits_at_k, precision_at_k=precision_at_k)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c37488d-0f2a-40ca-8cff-8550e1f0c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain_experiment().write_csv(\"results/markov_chain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391e614-4aba-4685-99ac-7100294fb221",
   "metadata": {},
   "source": [
    "## Transformer Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b339e69-fb8e-4776-9889-c5043736c7c9",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbfe155f-3d9d-473d-8387-b2aecbc59ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(num_items=num_items):\n",
    "    X_train, y_train, y_test = get_clean_train_valid_eval()\n",
    "    train = join_x_y(X_train, y_train) \n",
    "    test = join_x_y(train, y_test)\n",
    "    column_ordering = X_train.collect_schema().names()\n",
    "    \n",
    "    assert len(train.collect()) + len(y_test.collect()) == total_len\n",
    "    assert len(test.collect()) == total_len\n",
    "    \n",
    "    train = train.group_by(\"user_id\").tail(num_items).select(column_ordering)\n",
    "    test = test.group_by(\"user_id\").tail(num_items).select(column_ordering)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "class SequentialRecommenderDataset(Dataset):\n",
    "    def __init__(self, data, num_items=num_items):\n",
    "        self.num_items = num_items\n",
    "        self.num_users = data.group_by(\"user_id\").len().collect().shape[0]\n",
    "        self.samples = []\n",
    "\n",
    "        data_lists = data.group_by(\"user_id\").agg(\n",
    "            pl.col(\"product_id\")\n",
    "            .tail(num_items)\n",
    "        )\n",
    "\n",
    "        # Matrix of num_users x item interactions\n",
    "        self.samples = [\n",
    "            (\n",
    "                torch.tensor(product_list, dtype=torch.long),\n",
    "            )\n",
    "            for product_list in data_lists.collect()[\"product_id\"].to_list()\n",
    "        ]\n",
    "        \n",
    "        assert len(self.samples) == self.num_users\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns the sequence of items and sequence of items shifted by 1 to predict \n",
    "        x = self.samples[idx, :-1]\n",
    "        y = self.samples[idx, 1:]\n",
    "        return x, y\n",
    "\n",
    "def get_transformer_datasets():\n",
    "    train, test = get_data()\n",
    "    train_dataset = SequentialRecommenderDataset(train)\n",
    "    test_dataset = SequentialRecommenderDataset(test)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c0e2a-45fc-4ac6-962e-7da7625f14d8",
   "metadata": {},
   "source": [
    "### Experiment 3: Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f95e66-9ec6-4015-870a-fc41346a662e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
