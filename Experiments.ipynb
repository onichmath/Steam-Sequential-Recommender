{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d63a9cc-5c1b-4847-b549-48125c57b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import implicit # For BPR\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pyarrow\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345054f-2407-469e-8571-34986573e130",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ac105b-4d1f-47cd-8e00-bf7aa2ffde5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "train_len = 715613\n",
    "val_len = 92112\n",
    "test_len = 92112\n",
    "total_len = train_len + val_len + test_len\n",
    "assert total_len == 899837\n",
    "num_users = 92112\n",
    "num_items = 10546\n",
    "k_vals = [10, 25, 50]\n",
    "embed_dim = 64\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "block_size = 64\n",
    "dropout = 0.5\n",
    "SEED = 42\n",
    "padding_idx = 0\n",
    "ignore_idx = -100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "def build_results_df(model_name, k_vals, hits_at_k, precision_at_k):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the results of the model, given lists of k_vals, hits_at_k, and precision_at_k\n",
    "    \"\"\"\n",
    "    return pl.DataFrame(\n",
    "        {\n",
    "            \"model\": [model_name] * len(k_vals),\n",
    "            \"k\": k_vals,\n",
    "            \"hits_at_k\": hits_at_k,\n",
    "            \"precision_at_k\": precision_at_k\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d0e66-85cb-42d8-86d4-4a88e2ddbdbf",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a051eb4c-ec56-4e9e-8838-15dcdf839e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_data():\n",
    "    \"\"\"\n",
    "    Returns the full dataset sorted by user_id\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"hours\", \"products\", \"found_funny\", \"hours\", \"text\"]\n",
    "    df = pl.scan_parquet(\"data/reviews.parquet\")\n",
    "    print(df.collect())\n",
    "\n",
    "def load_train_val_test_data(): \n",
    "    \"\"\"\n",
    "    Returns datasets sorted by user_id with idx of their initial position in the dataset\n",
    "    - X_train: All interactions except the last two per user\n",
    "    - y_valid: Second to last interaction per user\n",
    "    - y_test: Last interaction per user\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"hours\", \"products\", \"found_funny\", \"hours\", \"text\"]\n",
    "    full = pl.scan_parquet(\"data/sorted_reviews.parquet\").drop(exclude_cols).with_row_index(\"idx\")\n",
    "    users_with_less_than_2_interactions = full.group_by(\"mapped_user_id\").agg(pl.count(\"mapped_product_id\").alias(\"interaction_count\"))\n",
    "    users_with_less_than_2_interactions = users_with_less_than_2_interactions.filter(pl.col(\"interaction_count\") < 5)\n",
    "    \n",
    "    y = full.group_by(\"mapped_user_id\").tail(2)\n",
    "    X_train = full.join(y, on=\"idx\", how=\"anti\") # Up to two interactions from last per user\n",
    "\n",
    "    y_valid = y.group_by(\"mapped_user_id\").head(1) # 1 interaction from last per user\n",
    "    y_test = y.group_by(\"mapped_user_id\").tail(1) # Last interaction per user\n",
    "\n",
    "    assert total_len == len(full.collect())\n",
    "    assert train_len == len(X_train.collect())\n",
    "    assert val_len == len(y_valid.collect())\n",
    "    assert test_len == len(y_test.collect())\n",
    "\n",
    "    return X_train, y_valid, y_test\n",
    "\n",
    "def format_train_eval_data(df, cols_to_keep=[\"idx\", \"user_id\", \"product_id\"]): \n",
    "    \"\"\"\n",
    "    Formats the data to have the columns: idx, user_id, product_id\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"review_date\", \"mapped_user_id\", \"mapped_product_id\"]\n",
    "    return (\n",
    "        df\n",
    "        .with_columns(\n",
    "            pl.col(\"mapped_user_id\").alias(\"user_id\"),\n",
    "            pl.col(\"mapped_product_id\").alias(\"product_id\"),\n",
    "        )\n",
    "        .drop(exclude_cols)\n",
    "        .select(cols_to_keep)\n",
    "    )\n",
    "\n",
    "def get_clean_train_valid_eval(): \n",
    "    \"\"\" \n",
    "    Returns the datasets formatted for training and evaluation\n",
    "    - X_train: All interactions except the last two per user\n",
    "    - y_valid: Second to last interaction per user\n",
    "    - y_test: Last interaction per user\n",
    "    \"\"\"\n",
    "    X_train, y_valid, y_test = load_train_val_test_data()\n",
    "    X_train = format_train_eval_data(X_train)\n",
    "    y_valid = format_train_eval_data(y_valid)\n",
    "    y_test = format_train_eval_data(y_test)\n",
    "    assert total_len == len(X_train.collect()) + len(y_valid.collect()) + len(y_test.collect())\n",
    "    return X_train, y_valid, y_test\n",
    "\n",
    "def join_x_y(X, y):\n",
    "    \"\"\"\n",
    "    Joins the training and validation datasets on each user \n",
    "    - X_test: All interactions except the last one per user\n",
    "    \"\"\"\n",
    "    X_join = pl.concat([X, y], how=\"vertical\").sort(\"idx\")\n",
    "    assert len(X_join.collect()) == len(X.collect()) + len(y.collect())\n",
    "    return X_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f92d4-43ae-4127-a268-5775802331a7",
   "metadata": {},
   "source": [
    "## Baseline Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503f8fe-fe31-4933-ab44-9c87352e92cd",
   "metadata": {},
   "source": [
    "### Experiment 1: Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d57e3a6-a358-403a-851d-9072b421e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_popular_items(X, k=50): \n",
    "    \"\"\"\n",
    "    Returns the k most popular items in X\n",
    "    \"\"\"\n",
    "    popular_items = (\n",
    "        X\n",
    "        .group_by(\"product_id\")\n",
    "        .agg(pl.count(\"product_id\").alias(\"count\"))\n",
    "        .sort(\"count\", descending=True)\n",
    "        .head(k)\n",
    "    )\n",
    "    return popular_items\n",
    "    \n",
    "def predict_based_on_item_popularity(X_train, y_valid, k=50): \n",
    "    \"\"\"\n",
    "    Predicts the most popular items for each user\n",
    "    \"\"\"\n",
    "    X_test = join_x_y(X_train, y_valid)\n",
    "    top_k_items = get_k_most_popular_items(\n",
    "        X_test,\n",
    "        k)\n",
    "    assert len(top_k_items.collect()) == k\n",
    "    return top_k_items\n",
    "\n",
    "\n",
    "def popularity_baseline(k_vals = k_vals):\n",
    "    X_train, y_valid, y_test = get_clean_train_valid_eval()\n",
    "    \n",
    "    hits_at_k = []\n",
    "    precision_at_k = []\n",
    "    ndcg_at_k = []\n",
    "\n",
    "    for k in k_vals:\n",
    "        y_pred = predict_based_on_item_popularity(X_train, y_valid, k = k)\n",
    "        hits = len(y_pred.join(y_test, on=[\"product_id\"]).collect())\n",
    "\n",
    "        num_predictions = len(y_pred.collect())\n",
    "        number_of_users = len(y_test.collect()) \n",
    "\n",
    "        hits_at_k.append(hits / number_of_users)\n",
    "\n",
    "        # Number of hits divided by number of predictions\n",
    "        precision_at_k.append(hits / k / number_of_users)\n",
    "\n",
    "    results = build_results_df(\"popularity\", k_vals, hits_at_k, precision_at_k)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c73ed84b-5a1b-4551-961c-254725879390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>k</th><th>hits_at_k</th><th>precision_at_k</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;popularity&quot;</td><td>10</td><td>0.038866</td><td>0.003887</td></tr><tr><td>&quot;popularity&quot;</td><td>25</td><td>0.079729</td><td>0.003189</td></tr><tr><td>&quot;popularity&quot;</td><td>50</td><td>0.125228</td><td>0.002505</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌────────────┬─────┬───────────┬────────────────┐\n",
       "│ model      ┆ k   ┆ hits_at_k ┆ precision_at_k │\n",
       "│ ---        ┆ --- ┆ ---       ┆ ---            │\n",
       "│ str        ┆ i64 ┆ f64       ┆ f64            │\n",
       "╞════════════╪═════╪═══════════╪════════════════╡\n",
       "│ popularity ┆ 10  ┆ 0.038866  ┆ 0.003887       │\n",
       "│ popularity ┆ 25  ┆ 0.079729  ┆ 0.003189       │\n",
       "│ popularity ┆ 50  ┆ 0.125228  ┆ 0.002505       │\n",
       "└────────────┴─────┴───────────┴────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popularity_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89146ddb-b548-4ea1-90b5-2b2f12800ae2",
   "metadata": {},
   "source": [
    "### Experiment 2: Markov Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8bb0b90-03f9-4b26-afad-508226af3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix_from_markov_chain(X):\n",
    "    \"\"\"\n",
    "    Builds a transition matrix from transition counts\n",
    "    OLD Method, stopped using it since pivots cannot be Lazy\n",
    "    \"\"\"\n",
    "    return (\n",
    "            X.collect()\n",
    "            .pivot(\n",
    "                on=\"prev_product_id\",\n",
    "                index=\"product_id\",\n",
    "                values=\"transitions\",\n",
    "                aggregate_function=\"sum\"\n",
    "            )\n",
    "            .fill_null(0)\n",
    "            .rename({\"product_id\": \"next_product_id\"})\n",
    "            .with_columns(pl.all().exclude(\"next_product_id\") / pl.all().sum())\n",
    "            .lazy()\n",
    "        )\n",
    "\n",
    "def get_matrix_shifted_by_3(X):\n",
    "    \"\"\"\n",
    "    Returns matrix shifted by 3\n",
    "    \"\"\"\n",
    "    return (\n",
    "        X.with_columns([\n",
    "            pl.col(\"product_id\").shift(1).alias(\"prev_1_product_id\"),\n",
    "            pl.col(\"user_id\").shift(1).alias(\"prev_1_user_id\"),\n",
    "            pl.col(\"product_id\").shift(2).alias(\"prev_2_product_id\"),\n",
    "            pl.col(\"user_id\").shift(2).alias(\"prev_2_user_id\"),\n",
    "            pl.col(\"product_id\").shift(3).alias(\"prev_3_product_id\"),\n",
    "            pl.col(\"user_id\").shift(3).alias(\"prev_3_user_id\"),\n",
    "        ])\n",
    "        .filter(pl.col(\"prev_3_product_id\").is_not_null())\n",
    "        .filter(pl.col(\"user_id\") == pl.col(\"prev_3_user_id\"))\n",
    "    )\n",
    "\n",
    "def get_previous_n_transitions(X, n=1):\n",
    "    \"\"\"\n",
    "    Returns the previous n transitions for each user weighted by how far back they are\n",
    "    \"\"\"\n",
    "    prev_column = f\"prev_{n}_product_id\"\n",
    "    return (\n",
    "        X\n",
    "        .group_by([prev_column, \"product_id\"])\n",
    "        .len().rename({\"len\": \"transitions\"})\n",
    "        .with_columns([pl.col(\"transitions\") / n])\n",
    "        .rename({prev_column: \"prev_product_id\"})\n",
    "    )\n",
    "\n",
    "def get_3rd_order_markov_chain(X):\n",
    "    \"\"\"\n",
    "    Returns the transition probabilities for a 3rd order Markov chain\n",
    "    \"\"\"\n",
    "    X_shifted = get_matrix_shifted_by_3(X)\n",
    "    \n",
    "    n_1_transition_counts = get_previous_n_transitions(X_shifted, n=1)\n",
    "    n_2_transition_counts = get_previous_n_transitions(X_shifted, n=2)\n",
    "    n_3_transition_counts = get_previous_n_transitions(X_shifted, n=3)\n",
    "\n",
    "    transition_counts = pl.concat([n_1_transition_counts, n_2_transition_counts, n_3_transition_counts])\n",
    "    \n",
    "    transition_probs = (\n",
    "        transition_counts\n",
    "        .with_columns(pl.col(\"transitions\") / pl.col(\"transitions\").sum().over(\"prev_product_id\"))\n",
    "        .rename({\n",
    "            \"transitions\": \"probability\",\n",
    "            \"product_id\": \"next_product_id\",\n",
    "            \"prev_product_id\": \"product_id\",\n",
    "            })\n",
    "    )\n",
    "    \n",
    "    return transition_probs\n",
    "\n",
    "def predict_based_on_markov_chain(y, transition_matrix, k=50):\n",
    "    \"\"\"\n",
    "    Predicts the next k items based on the transition matrix\n",
    "    \"\"\"\n",
    "    y_pred =  (\n",
    "        y\n",
    "        .drop(\"idx\")\n",
    "        .join(transition_matrix, on=\"product_id\", how=\"left\")\n",
    "        .group_by(\"user_id\")\n",
    "        .agg(\n",
    "            [\n",
    "                pl.struct([\"next_product_id\", \"probability\"])\n",
    "                .sort_by(\"probability\", descending=True)\n",
    "                .slice(0, k)\n",
    "                .alias(\"top_predictions\")\n",
    "            ]\n",
    "        )\n",
    "        .explode(\"top_predictions\")\n",
    "        .select(\n",
    "            \"user_id\",\n",
    "            pl.col(\"top_predictions\").struct.field(\"next_product_id\").alias(\"product_id\"),\n",
    "            pl.col(\"top_predictions\").struct.field(\"probability\").alias(\"probability\"),\n",
    "        )\n",
    "    )\n",
    "    return y_pred\n",
    "\n",
    "def markov_chain_experiment(k_vals = k_vals):\n",
    "    X_train, y_valid, y_test = get_clean_train_valid_eval()\n",
    "    X_test = join_x_y(X_train, y_valid)\n",
    "\n",
    "    transition_matrix = get_3rd_order_markov_chain(X_test)\n",
    "    hits_at_k = []\n",
    "    precision_at_k = []\n",
    "\n",
    "    for k in k_vals:\n",
    "        y_pred = predict_based_on_markov_chain(y_valid, transition_matrix, k = k)\n",
    "        hits = len(y_pred.join(y_test, on=[\"user_id\", \"product_id\"]).collect())\n",
    "\n",
    "        num_predictions = len(y_pred.collect())\n",
    "        number_of_users = len(y_test.collect()) \n",
    "\n",
    "        hits_at_k.append(hits / number_of_users)\n",
    "        precision_at_k.append(hits / k / number_of_users)\n",
    "\n",
    "    results = build_results_df(\"Markov Chain\", k_vals=k_vals, hits_at_k=hits_at_k, precision_at_k=precision_at_k)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37488d-0f2a-40ca-8cff-8550e1f0c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391e614-4aba-4685-99ac-7100294fb221",
   "metadata": {},
   "source": [
    "## Transformer Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c0e2a-45fc-4ac6-962e-7da7625f14d8",
   "metadata": {},
   "source": [
    "### Experiment 3: Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b339e69-fb8e-4776-9889-c5043736c7c9",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbfe155f-3d9d-473d-8387-b2aecbc59ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(block_size=block_size):\n",
    "    X_train, y_train, y_test = get_clean_train_valid_eval()\n",
    "    train = join_x_y(X_train, y_train) \n",
    "    test = join_x_y(train, y_test)\n",
    "    column_ordering = X_train.collect_schema().names()\n",
    "    \n",
    "    assert len(train.collect()) + len(y_test.collect()) == total_len\n",
    "    assert len(test.collect()) == total_len\n",
    "    \n",
    "    train = train.group_by(\"user_id\").tail(block_size).select(column_ordering)\n",
    "    test = test.group_by(\"user_id\").tail(block_size).select(column_ordering)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "class SequentialRecommenderDataset(Dataset):\n",
    "    def __init__(self, data, block_size=block_size):\n",
    "        self.block_size = block_size\n",
    "        self.num_users = data.group_by(\"user_id\").len().collect().shape[0]\n",
    "\n",
    "        samples = (\n",
    "            data\n",
    "            .group_by(\"user_id\")\n",
    "            .agg(pl.col(\"product_id\"))\n",
    "            .collect()\n",
    "            .to_dict()\n",
    "        )\n",
    "        self.samples = {}\n",
    "        self.user_ids = []\n",
    "        \n",
    "        for i, (u_id, p_ids) in enumerate(zip(samples[\"user_id\"], samples[\"product_id\"])):\n",
    "            self.samples[int(u_id)] = torch.tensor(p_ids, dtype=torch.long)\n",
    "            self.user_ids.append(int(u_id))\n",
    "            \n",
    "        assert len(self.user_ids) == self.num_users\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns the sequence of items and sequence of items shifted by 1 to predict\n",
    "        item_sequence = self.samples[self.user_ids[idx]]\n",
    "        x = item_sequence[:-1]\n",
    "        y = item_sequence[1:]\n",
    "        return x, y\n",
    "\n",
    "def padding_collate(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    last_items_idx = torch.tensor([len(seq) - 1 for seq in sequences], dtype=int)\n",
    "    \n",
    "    # THIS IS TERRIBLE, THIS WILL LIKELY CAUSE AN ERORR LATER WHEN I FORGET ABOUT SCOPE\n",
    "    max_len = block_size\n",
    "\n",
    "    padded_sequences = torch.stack([\n",
    "        torch.nn.functional.pad(seq, (0, max_len - seq.size(0)), \"constant\", padding_idx)\n",
    "        for seq in sequences\n",
    "    ])\n",
    "\n",
    "    padded_labels = torch.stack([\n",
    "        torch.nn.functional.pad(label, (0, max_len - label.size(0)), \"constant\", ignore_idx)\n",
    "        for label in labels\n",
    "    ])\n",
    "\n",
    "    # # Find pred indices \n",
    "    # mask = (padded_labels != ignore_idx)  # (B, T)\n",
    "\n",
    "    # pred_idx = mask.long().cumsum(dim=1) - 1\n",
    "    # pred_idx = pred_idx.gather(1, mask.sum(dim=1, keepdim=True) - 1).squeeze(1)\n",
    "    # pred_idx[mask.sum(dim=1) == 0] = -1\n",
    "\n",
    "    return padded_sequences, padded_labels, last_items_idx\n",
    "\n",
    "def get_transformer_datasets():\n",
    "    train, test = get_data()\n",
    "    train_dataset = SequentialRecommenderDataset(train)\n",
    "    test_dataset = SequentialRecommenderDataset(test)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c48a7-b6b2-4d88-be23-e4379a5946dc",
   "metadata": {},
   "source": [
    "#### Transformer Classes\n",
    "Transformer classes based off work from Matthew O'Malley-Nichols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0587327c-77f7-453c-a5ae-42425a8d1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBase(nn.Module):\n",
    "    def __init__(self, embed_dim, block_size, head_dim, autoregression, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim \n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.autoregression = autoregression\n",
    "\n",
    "        # Channels x Head size\n",
    "        self.query_linear = nn.Linear(self.embed_dim, self.head_dim, bias=False)\n",
    "        self.key_linear = nn.Linear(self.embed_dim, self.head_dim, bias=False)\n",
    "        self.value_linear = nn.Linear(self.embed_dim, self.head_dim, bias=False)\n",
    "\n",
    "        if self.autoregression:\n",
    "            self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def get_shape(self, x):\n",
    "        B, T, C = x.shape\n",
    "        assert C == self.embed_dim\n",
    "        return B, T, C\n",
    "\n",
    "    def project_linear_components(self, x):\n",
    "        # Get the linearly projected queries, keys, and values\n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "        return query, key, value\n",
    "\n",
    "    def mask_causal_logits(self, logits, T):\n",
    "        # Mask out future tokens if decoder\n",
    "        if self.autoregression:\n",
    "            logits = logits.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        return logits\n",
    "\n",
    "    def compute_attention(self, logits, value):\n",
    "        weights = F.softmax(logits, dim=-1)\n",
    "        regularized_weights = self.dropout(weights)\n",
    "        attention = torch.einsum(\"btt,btc->btc\", regularized_weights, value)\n",
    "        return attention, weights\n",
    "\n",
    "\n",
    "class SelfAttentionHead(SelfAttentionBase):\n",
    "    # Single head of attention based off \"Let's build GPT: from scratch, in code, spelled out\" by Andrej Karpathy\n",
    "    # and the \"Attention is All You Need\" paper\n",
    "    def __init__(self, embed_dim, block_size, head_dim, autoregression, dropout):\n",
    "        super().__init__(embed_dim=embed_dim, \n",
    "                         block_size=block_size, \n",
    "                         head_dim=head_dim, \n",
    "                         autoregression=autoregression, \n",
    "                         dropout=dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Forward pass for basic self attention\n",
    "        B, T, C = self.get_shape(x)\n",
    "        q, k, v = self.project_linear_components(x)\n",
    "\n",
    "        # Compute attention weights\n",
    "        logits = torch.einsum(\"btc,bTc->bTt\", q, k) # B x T x C @ B x T x C -> B x T x T\n",
    "        logits = logits / (C ** 0.5) # Divide by sqrt(d_k) to prevent peaky softmax\n",
    "\n",
    "        logits = logits.masked_fill(attention_mask, float(\"-inf\"))\n",
    "        logits = self.mask_causal_logits(logits, T)\n",
    "        attention, weights = self.compute_attention(logits, v)\n",
    "\n",
    "        return attention, weights \n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Multiple heads of attention based off \"Let's build GPT: from scratch, in code, spelled out\" by Andrej Karpathy\n",
    "    # and the \"Attention is All You Need\" paper\n",
    "    def __init__(self, num_heads, embed_dim, block_size, autoregression, attention, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self._init_heads(num_heads, embed_dim, block_size, autoregression, attention, dropout)\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _init_heads(self, num_heads, embed_dim, block_size, autoregression, attention, dropout):\n",
    "        head_dim = embed_dim // num_heads\n",
    "        params = {\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"block_size\": block_size,\n",
    "            \"head_dim\": head_dim,\n",
    "            \"autoregression\": autoregression,\n",
    "            \"dropout\": dropout\n",
    "            }\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(**params) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Multihead attention based off \"Attention is All You Need\" paper\n",
    "        attention_maps = []\n",
    "        attentions = []\n",
    "        for head in self.heads:\n",
    "            attention_output, attention_map = head(x, attention_mask)\n",
    "            attention_maps.append(attention_map)\n",
    "            attentions.append(attention_output)\n",
    "\n",
    "        concatenated_attention = torch.cat(attentions, dim=-1)\n",
    "        concatenated_attention = self.dropout(self.proj(concatenated_attention))\n",
    "        return concatenated_attention, attention_maps\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # Feed forward network based off \"Let's build GPT: from scratch, in code, spelled out\" by Andrej Karpathy\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff_net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff_net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    # Single transformer block based off \"Attention is All You Need\" paper\n",
    "    def __init__(self, embed_dim, num_heads, block_size, hidden_dim, autoregression, attention, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, \n",
    "                                            embed_dim=embed_dim, \n",
    "                                            block_size=block_size, \n",
    "                                            autoregression=autoregression, \n",
    "                                            attention=attention, \n",
    "                                            dropout=dropout)\n",
    "\n",
    "        self.feed_forward = FeedForward(embed_dim=embed_dim, \n",
    "                                        hidden_dim=hidden_dim, \n",
    "                                        dropout=dropout)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim) # Pre-normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Residual connection around each sub-block\n",
    "        attentions, attention_maps = self.attention(self.layer_norm1(x), attention_mask)\n",
    "        with torch.no_grad(): # Discard gradients for the attention maps\n",
    "            attention_maps = [attn_map.detach() for attn_map in attention_maps]\n",
    "        x = x + attentions\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x, attention_maps\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, block_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Embedding(block_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        return self.positional_embedding(torch.arange(T, device=x.device))\n",
    "\n",
    "class TransformerBase(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, num_heads, num_layers, hidden_dim, autoregression, attention=\"basic\", position_encoding=\"learned\", dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.position_encoding = LearnedPositionalEncoding(block_size, embed_dim)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size+1, embed_dim, padding_idx=0)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, block_size, hidden_dim, autoregression, attention, dropout) \n",
    "            for _ in range(num_layers)\n",
    "            ])\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            # nn.init.kaiming_normal_(module.weight)\n",
    "            # Normal distribution reduces train perplexity to be within range, at cost of test perplexity\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "\n",
    "    def embed(self, x):\n",
    "        # Return the embeddings for the input sequence\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.token_embedding_table(x)\n",
    "        if not self.position_encoding:\n",
    "            return token_embeddings\n",
    "        pos_embeddings = self.position_encoding(x)\n",
    "        return token_embeddings + pos_embeddings\n",
    "\n",
    "class Decoder(TransformerBase):\n",
    "    def __init__(self, vocab_size=num_items, embed_dim=embed_dim, block_size=block_size, \n",
    "                 num_heads=2, num_layers=4, hidden_dim=embed_dim*4, \n",
    "                 attention=\"basic\", position_encoding=\"learned\", dropout=0.0, neg_ce_loss=False):\n",
    "        super().__init__(vocab_size=vocab_size,\n",
    "                         embed_dim=embed_dim,\n",
    "                         block_size=block_size,\n",
    "                         num_heads=num_heads,\n",
    "                         num_layers=num_layers,\n",
    "                         hidden_dim=hidden_dim,\n",
    "                         autoregression=True,\n",
    "                         position_encoding=position_encoding,\n",
    "                         attention=attention,\n",
    "                         dropout=dropout)\n",
    "        self.neg_ce_loss = neg_ce_loss\n",
    "        self.rec_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        attention_mask = x == 0 # B x T  \n",
    "        attention_mask = attention_mask.unsqueeze(1)  # Shape: (B, 1, T)\n",
    "        attention_mask = attention_mask.expand(-1, self.block_size, -1)  # Shape: (B, T, T)\n",
    "\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        attention_maps = []\n",
    "        for block in self.blocks:\n",
    "            x, maps = block(x, attention_mask)\n",
    "            attention_maps.extend(maps)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.rec_head(x) # B x T x V\n",
    "\n",
    "        cross_entropy_loss = None\n",
    "        if y is not None:\n",
    "            logits_flat = logits.view(-1, logits.size(-1))\n",
    "            y_flat = y.view(-1)\n",
    "            cross_entropy_loss = F.cross_entropy(logits_flat, y_flat, ignore_index=ignore_idx)\n",
    "            if self.neg_ce_loss:\n",
    "                with torch.no_grad():\n",
    "                    B = x.size(0)\n",
    "                    for i in range(B):\n",
    "                        neg_mask = torch.arange(B) != i\n",
    "                        neg_logits = logits[neg_mask]\n",
    "                        neg_loss = -torch.mean(torch.log_softmax(neg_logits.view(-1, neg_logits.size(-1)), dim=-1).sum(dim=-1) + 1e-12)\n",
    "                        cross_entropy_loss += neg_loss\n",
    "\n",
    "        return logits, cross_entropy_loss, attention_maps\n",
    "\n",
    "    def get_batch_metrics_at_k(self, logits, Y, last_items_idx, k):\n",
    "        with torch.no_grad():\n",
    "            row_indices = torch.arange(batch_size)[:len(Y)]\n",
    "            last_targets = Y[row_indices, last_items_idx] \n",
    "            \n",
    "            pred = logits[row_indices, last_items_idx, :]\n",
    "\n",
    "            prob = F.softmax(pred, dim=-1)\n",
    "            \n",
    "            top_k_values, top_k_indices = torch.topk(prob, k=k, dim=-1)\n",
    "            \n",
    "            hits = (last_targets.unsqueeze(1) == top_k_indices)  # Shape: [B, k]\n",
    "            hit_count = hits.any(dim=1).sum().item()\n",
    "            hit_at_k = hit_count / batch_size\n",
    "\n",
    "            dcg = (hits.float() / torch.log2(torch.arange(2, k + 2, device=logits.device).float())).sum(dim=1)\n",
    "            ideal_relevance = torch.ones(len(row_indices), k, device=logits.device)\n",
    "            idcg = (ideal_relevance / torch.log2(torch.arange(2, k + 2, device=logits.device).float())).sum(dim=1)\n",
    "            \n",
    "            ndcg = (dcg / idcg)\n",
    "            ndcg[torch.isnan(ndcg)] = 0.0\n",
    "            ndcg = ndcg.mean().item()\n",
    "        \n",
    "            return hit_at_k, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c06c1-d505-42a7-a42c-d9b79ed4e9fa",
   "metadata": {},
   "source": [
    "#### Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a96f4fc7-b1f1-4b70-9db9-1442400e8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(num_epochs=50, batch_size=32, num_items=num_items, learning_rate=learning_rate, k_vals=k_vals, model_name=\"decoder_ce\"):\n",
    "    # Overfit quickly with num_heads =2 and num_layers =4 w/ 1e-3 learning rate\n",
    "    train_dataset, test_dataset = get_transformer_datasets()\n",
    "    del test_dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=padding_collate)\n",
    "    #test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=padding_collate)\n",
    "    model = Decoder(dropout=dropout, neg_ce_loss=True)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(device)\n",
    "    \n",
    "    model.train()\n",
    "    max_hit = -np.inf\n",
    "    patience = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        total_hits = 0\n",
    "        print(epoch)\n",
    "        for batch, (X, Y, last_items_idx) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)  \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss, _ = model(X, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                pass\n",
    "                #last_items_idx.to(device)\n",
    "                #hit, ncdg = model.get_batch_metrics_at_k(logits, Y, last_items_idx, 50)\n",
    "                #total_hits += hit\n",
    "                # print(f\"Epoch {epoch}, Batch {batch}, Loss {loss}\")\n",
    "                # print(f\"Epoch {epoch}, Batch {batch}, hit@k {hit, ncdg}\")\n",
    "        if total_hits < max_hit:\n",
    "            patience -= 1 \n",
    "        else:\n",
    "            min_hit = total_hits\n",
    "        if patience <= 0:\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), f\"./models/{model_name}_{epoch}.pth\")\n",
    "\n",
    "train_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6609f-aaea-48c6-831e-8e4c74672849",
   "metadata": {},
   "source": [
    "#### Transformer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff247ad8-3629-4073-b5c3-163504c8a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3961/1665259353.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(state_dict_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>k</th><th>hits_at_k</th><th>ncdg_at_k</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;decoder_bce_50_5&quot;</td><td>10</td><td>0.07615</td><td>0.008624</td></tr><tr><td>&quot;decoder_bce_50_5&quot;</td><td>25</td><td>0.14069</td><td>0.00674</td></tr><tr><td>&quot;decoder_bce_50_5&quot;</td><td>50</td><td>0.21326</td><td>0.005329</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌──────────────────┬─────┬───────────┬───────────┐\n",
       "│ model            ┆ k   ┆ hits_at_k ┆ ncdg_at_k │\n",
       "│ ---              ┆ --- ┆ ---       ┆ ---       │\n",
       "│ str              ┆ i64 ┆ f64       ┆ f64       │\n",
       "╞══════════════════╪═════╪═══════════╪═══════════╡\n",
       "│ decoder_bce_50_5 ┆ 10  ┆ 0.07615   ┆ 0.008624  │\n",
       "│ decoder_bce_50_5 ┆ 25  ┆ 0.14069   ┆ 0.00674   │\n",
       "│ decoder_bce_50_5 ┆ 50  ┆ 0.21326   ┆ 0.005329  │\n",
       "└──────────────────┴─────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_eval(model_str=\"decoder_bce_50_5\", k_vals=k_vals, batch_size=batch_size, num_items=num_items):\n",
    "    train_dataset, test_dataset = get_transformer_datasets()\n",
    "    del train_dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=padding_collate)\n",
    "\n",
    "    state_dict_path = f\"./models/{model_str}.pth\"\n",
    "    model = Decoder(dropout=dropout)\n",
    "    model.load_state_dict(torch.load(state_dict_path))\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    results = {}\n",
    "\n",
    "    hits_at_k = [0.0 for k in k_vals]\n",
    "    ncdg_at_k = [0.0 for k in k_vals]\n",
    "\n",
    "    num_batches = 0 \n",
    "    for batch, (X, Y, last_items_idx) in enumerate(test_loader):\n",
    "        num_batches += 1\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)  \n",
    "        last_items_idx = last_items_idx.to(device)\n",
    "        logits, _, _ = model(X)\n",
    "\n",
    "        for i, k in enumerate(k_vals):\n",
    "            hit, ncdg = model.get_batch_metrics_at_k(logits, Y, last_items_idx, k)\n",
    "            hits_at_k[i] += hit\n",
    "            ncdg_at_k[i] += ncdg\n",
    "\n",
    "    ncdg_at_k = np.array(ncdg_at_k) / num_batches \n",
    "    hits_at_k = np.array(hits_at_k) / num_batches \n",
    "\n",
    "    return pl.DataFrame(\n",
    "        {\n",
    "            \"model\": [f\"{model_str}\"] * len(k_vals),\n",
    "            \"k\": k_vals,\n",
    "            \"hits_at_k\": hits_at_k,\n",
    "            \"ncdg_at_k\": ncdg_at_k\n",
    "        }\n",
    "    )\n",
    "transformer_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1189363f-3b80-4b60-b0f9-087f1adfaee1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6662f7ad-43e5-443c-a99a-6b31bb5770da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_results():\n",
    "    bert4rec = pl.read_csv(\"./results/bert4rec.csv\").drop(\"precision_at_k\")\n",
    "    decoder_ce_15 = pl.read_csv(\"./results/decoder_ce_15.csv\").drop(\"ncdg_at_k\").with_columns(pl.col(\"model\").replace(\"decoder_ce_50_15\", \"D-CE\"))\n",
    "    decoder_bce_15 = pl.read_csv(\"./results/decoder_bce_15.csv\").drop(\"ncdg_at_k\").with_columns(pl.col(\"model\").replace(\"decoder_bce_50_15\", \"D-CCE\"))\n",
    "    popularity = pl.read_csv(\"./results/popularity_baseline.csv\").drop(\"precision_at_k\").with_columns(pl.col(\"model\").replace(\"popularity\", \"PopRec\"))\n",
    "    markov_chain = pl.read_csv(\"./results/markov_chain.csv\").drop(\"precision_at_k\")\n",
    "    results = pl.concat([bert4rec, decoder_ce_15, decoder_bce_15, popularity, markov_chain]).filter(pl.col(\"k\") > 5)\n",
    "    # Round all to 4 decimal places \n",
    "    results = (\n",
    "        results\n",
    "        .with_columns(\n",
    "            pl.col(\"hits_at_k\").round(4),\n",
    "        )\n",
    "        .pivot(on=\"k\", index=\"model\", values=\"hits_at_k\")\n",
    "        .sort(\"50\", descending=True)\n",
    "    )\n",
    "    return results\n",
    "\n",
    "concat_results().write_csv(\"./results/total_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6b9f2ab-df31-4cce-8d83-5b6b595df73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyperparameters():\n",
    "    hp = pl.DataFrame(\n",
    "        {\n",
    "            \"Hyperparameter\": [\"Batch Size\",\n",
    "                               \"Block Size\", \n",
    "                               \"Dropout\",\n",
    "                               \"Embedding Dim\", \n",
    "                               \"Epochs\",\n",
    "                               \"Hidden Dim\", \n",
    "                               \"Learning Rate\", ],\n",
    "            \"Bert4Rec\": [256, 50, 0.5, 64, 5, 64, 0.001], \n",
    "            \"D-CE\": [128, 64, 0.5, 64, 15, 256, 0.001],\n",
    "            \"D-CCE\": [32, 64, 0.5, 64, 15, 256, 0.001],\n",
    "        }, strict=False\n",
    "    )\n",
    "    return hp\n",
    "print_hyperparameters().write_csv(\"./results/hyperparameters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58751172-3566-4f13-a5e6-9ba309f82cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_hits_at_k():\n",
    "    data = pl.read_csv(\"./results/total_results.csv\")\n",
    "    models = data[\"model\"].to_list()\n",
    "    categories = [\"10\", \"25\", \"50\"]\n",
    "    values = data[categories].to_numpy()\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.15\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for i, (model, values_row) in enumerate(zip(models, values)):\n",
    "        ax.bar(x + i * width, values_row, width, label=model)\n",
    "    \n",
    "    ax.set_xlabel('Hit@k', fontsize=12)\n",
    "    ax.set_ylabel('Performance', fontsize=12)\n",
    "    ax.set_title('Model Performance by Hit@k', fontsize=14)\n",
    "    ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.legend(title=\"Model\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./results/hits_at_k.png\")\n",
    "    \n",
    "visualize_hits_at_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e90f5e-3354-42a3-a24d-8ef5213acc23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
