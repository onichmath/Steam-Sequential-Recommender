{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d63a9cc-5c1b-4847-b549-48125c57b789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthewo/Documents/College/CSE258/ass2/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import implicit # For BPR\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pyarrow\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345054f-2407-469e-8571-34986573e130",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ac105b-4d1f-47cd-8e00-bf7aa2ffde5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "train_len = 715613\n",
    "val_len = 92112\n",
    "test_len = 92112\n",
    "total_len = train_len + val_len + test_len\n",
    "assert total_len == 899837\n",
    "num_users = 92112\n",
    "num_items = 10546\n",
    "k_vals = [10, 25, 50]\n",
    "embed_dim = 64\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "block_size = 64\n",
    "dropout = 0.5\n",
    "SEED = 42\n",
    "padding_idx = 0\n",
    "ignore_idx = -100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "def build_results_df(model_name, k_vals, hits_at_k, precision_at_k):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the results of the model, given lists of k_vals, hits_at_k, and precision_at_k\n",
    "    \"\"\"\n",
    "    return pl.DataFrame(\n",
    "        {\n",
    "            \"model\": [model_name] * len(k_vals),\n",
    "            \"k\": k_vals,\n",
    "            \"hits_at_k\": hits_at_k,\n",
    "            \"precision_at_k\": precision_at_k\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d0e66-85cb-42d8-86d4-4a88e2ddbdbf",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a051eb4c-ec56-4e9e-8838-15dcdf839e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_data():\n",
    "    \"\"\"\n",
    "    Returns the full dataset sorted by user_id\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"hours\", \"products\", \"found_funny\", \"hours\", \"text\"]\n",
    "    df = pl.scan_parquet(\"data/reviews.parquet\")\n",
    "    print(df.collect())\n",
    "\n",
    "def load_train_val_test_data(): \n",
    "    \"\"\"\n",
    "    Returns datasets sorted by user_id with idx of their initial position in the dataset\n",
    "    - X_train: All interactions except the last two per user\n",
    "    - y_valid: Second to last interaction per user\n",
    "    - y_test: Last interaction per user\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"hours\", \"products\", \"found_funny\", \"hours\", \"text\"]\n",
    "    full = pl.scan_parquet(\"data/sorted_reviews.parquet\").drop(exclude_cols).with_row_index(\"idx\")\n",
    "    users_with_less_than_2_interactions = full.group_by(\"mapped_user_id\").agg(pl.count(\"mapped_product_id\").alias(\"interaction_count\"))\n",
    "    users_with_less_than_2_interactions = users_with_less_than_2_interactions.filter(pl.col(\"interaction_count\") < 5)\n",
    "    \n",
    "    y = full.group_by(\"mapped_user_id\").tail(2)\n",
    "    X_train = full.join(y, on=\"idx\", how=\"anti\") # Up to two interactions from last per user\n",
    "\n",
    "    y_valid = y.group_by(\"mapped_user_id\").head(1) # 1 interaction from last per user\n",
    "    y_test = y.group_by(\"mapped_user_id\").tail(1) # Last interaction per user\n",
    "\n",
    "    assert total_len == len(full.collect())\n",
    "    assert train_len == len(X_train.collect())\n",
    "    assert val_len == len(y_valid.collect())\n",
    "    assert test_len == len(y_test.collect())\n",
    "\n",
    "    return X_train, y_valid, y_test\n",
    "\n",
    "def format_train_eval_data(df, cols_to_keep=[\"idx\", \"user_id\", \"product_id\"]): \n",
    "    \"\"\"\n",
    "    Formats the data to have the columns: idx, user_id, product_id\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"review_date\", \"mapped_user_id\", \"mapped_product_id\"]\n",
    "    return (\n",
    "        df\n",
    "        .with_columns(\n",
    "            pl.col(\"mapped_user_id\").alias(\"user_id\"),\n",
    "            pl.col(\"mapped_product_id\").alias(\"product_id\"),\n",
    "        )\n",
    "        .drop(exclude_cols)\n",
    "        .select(cols_to_keep)\n",
    "    )\n",
    "\n",
    "def get_clean_train_valid_eval(): \n",
    "    \"\"\" \n",
    "    Returns the datasets formatted for training and evaluation\n",
    "    - X_train: All interactions except the last two per user\n",
    "    - y_valid: Second to last interaction per user\n",
    "    - y_test: Last interaction per user\n",
    "    \"\"\"\n",
    "    X_train, y_valid, y_test = load_train_val_test_data()\n",
    "    X_train = format_train_eval_data(X_train)\n",
    "    y_valid = format_train_eval_data(y_valid)\n",
    "    y_test = format_train_eval_data(y_test)\n",
    "    assert total_len == len(X_train.collect()) + len(y_valid.collect()) + len(y_test.collect())\n",
    "    return X_train, y_valid, y_test\n",
    "\n",
    "def join_x_y(X, y):\n",
    "    \"\"\"\n",
    "    Joins the training and validation datasets on each user \n",
    "    - X_test: All interactions except the last one per user\n",
    "    \"\"\"\n",
    "    X_join = pl.concat([X, y], how=\"vertical\").sort(\"idx\")\n",
    "    assert len(X_join.collect()) == len(X.collect()) + len(y.collect())\n",
    "    return X_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f92d4-43ae-4127-a268-5775802331a7",
   "metadata": {},
   "source": [
    "## Baseline Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503f8fe-fe31-4933-ab44-9c87352e92cd",
   "metadata": {},
   "source": [
    "### Experiment 1: Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d57e3a6-a358-403a-851d-9072b421e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_popular_items(X, k=50): \n",
    "    \"\"\"\n",
    "    Returns the k most popular items in X\n",
    "    \"\"\"\n",
    "    popular_items = (\n",
    "        X\n",
    "        .group_by(\"product_id\")\n",
    "        .agg(pl.count(\"product_id\").alias(\"count\"))\n",
    "        .sort(\"count\", descending=True)\n",
    "        .head(k)\n",
    "    )\n",
    "    return popular_items\n",
    "    \n",
    "def predict_based_on_item_popularity(X_train, y_valid, k=50): \n",
    "    \"\"\"\n",
    "    Predicts the most popular items for each user\n",
    "    \"\"\"\n",
    "    X_test = join_x_y(X_train, y_valid)\n",
    "    top_k_items = get_k_most_popular_items(\n",
    "        X_test,\n",
    "        k)\n",
    "    assert len(top_k_items.collect()) == k\n",
    "    return top_k_items\n",
    "\n",
    "\n",
    "def popularity_baseline(k_vals = k_vals):\n",
    "    X_train, y_valid, y_test = get_clean_train_valid_eval()\n",
    "    \n",
    "    hits_at_k = []\n",
    "    precision_at_k = []\n",
    "    ndcg_at_k = []\n",
    "\n",
    "    for k in k_vals:\n",
    "        y_pred = predict_based_on_item_popularity(X_train, y_valid, k = k)\n",
    "        hits = len(y_pred.join(y_test, on=[\"product_id\"]).collect())\n",
    "\n",
    "        num_predictions = len(y_pred.collect())\n",
    "        number_of_users = len(y_test.collect()) \n",
    "\n",
    "        hits_at_k.append(hits / number_of_users)\n",
    "\n",
    "        # Number of hits divided by number of predictions\n",
    "        precision_at_k.append(hits / k / number_of_users)\n",
    "\n",
    "    results = build_results_df(\"popularity\", k_vals, hits_at_k, precision_at_k)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c73ed84b-5a1b-4551-961c-254725879390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>k</th><th>hits_at_k</th><th>precision_at_k</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;popularity&quot;</td><td>10</td><td>0.038866</td><td>0.003887</td></tr><tr><td>&quot;popularity&quot;</td><td>25</td><td>0.079729</td><td>0.003189</td></tr><tr><td>&quot;popularity&quot;</td><td>50</td><td>0.125228</td><td>0.002505</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌────────────┬─────┬───────────┬────────────────┐\n",
       "│ model      ┆ k   ┆ hits_at_k ┆ precision_at_k │\n",
       "│ ---        ┆ --- ┆ ---       ┆ ---            │\n",
       "│ str        ┆ i64 ┆ f64       ┆ f64            │\n",
       "╞════════════╪═════╪═══════════╪════════════════╡\n",
       "│ popularity ┆ 10  ┆ 0.038866  ┆ 0.003887       │\n",
       "│ popularity ┆ 25  ┆ 0.079729  ┆ 0.003189       │\n",
       "│ popularity ┆ 50  ┆ 0.125228  ┆ 0.002505       │\n",
       "└────────────┴─────┴───────────┴────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popularity_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89146ddb-b548-4ea1-90b5-2b2f12800ae2",
   "metadata": {},
   "source": [
    "### Experiment 2: Markov Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8bb0b90-03f9-4b26-afad-508226af3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix_from_markov_chain(X):\n",
    "    \"\"\"\n",
    "    Builds a transition matrix from transition counts\n",
    "    OLD Method, stopped using it since pivots cannot be Lazy\n",
    "    \"\"\"\n",
    "    return (\n",
    "            X.collect()\n",
    "            .pivot(\n",
    "                on=\"prev_product_id\",\n",
    "                index=\"product_id\",\n",
    "                values=\"transitions\",\n",
    "                aggregate_function=\"sum\"\n",
    "            )\n",
    "            .fill_null(0)\n",
    "            .rename({\"product_id\": \"next_product_id\"})\n",
    "            .with_columns(pl.all().exclude(\"next_product_id\") / pl.all().sum())\n",
    "            .lazy()\n",
    "        )\n",
    "\n",
    "def get_matrix_shifted_by_3(X):\n",
    "    \"\"\"\n",
    "    Returns matrix shifted by 3\n",
    "    \"\"\"\n",
    "    return (\n",
    "        X.with_columns([\n",
    "            pl.col(\"product_id\").shift(1).alias(\"prev_1_product_id\"),\n",
    "            pl.col(\"user_id\").shift(1).alias(\"prev_1_user_id\"),\n",
    "            pl.col(\"product_id\").shift(2).alias(\"prev_2_product_id\"),\n",
    "            pl.col(\"user_id\").shift(2).alias(\"prev_2_user_id\"),\n",
    "            pl.col(\"product_id\").shift(3).alias(\"prev_3_product_id\"),\n",
    "            pl.col(\"user_id\").shift(3).alias(\"prev_3_user_id\"),\n",
    "        ])\n",
    "        .filter(pl.col(\"prev_3_product_id\").is_not_null())\n",
    "        .filter(pl.col(\"user_id\") == pl.col(\"prev_3_user_id\"))\n",
    "    )\n",
    "\n",
    "def get_previous_n_transitions(X, n=1):\n",
    "    \"\"\"\n",
    "    Returns the previous n transitions for each user weighted by how far back they are\n",
    "    \"\"\"\n",
    "    prev_column = f\"prev_{n}_product_id\"\n",
    "    return (\n",
    "        X\n",
    "        .group_by([prev_column, \"product_id\"])\n",
    "        .len().rename({\"len\": \"transitions\"})\n",
    "        .with_columns([pl.col(\"transitions\") / n])\n",
    "        .rename({prev_column: \"prev_product_id\"})\n",
    "    )\n",
    "\n",
    "def get_3rd_order_markov_chain(X):\n",
    "    \"\"\"\n",
    "    Returns the transition probabilities for a 3rd order Markov chain\n",
    "    \"\"\"\n",
    "    X_shifted = get_matrix_shifted_by_3(X)\n",
    "    \n",
    "    n_1_transition_counts = get_previous_n_transitions(X_shifted, n=1)\n",
    "    n_2_transition_counts = get_previous_n_transitions(X_shifted, n=2)\n",
    "    n_3_transition_counts = get_previous_n_transitions(X_shifted, n=3)\n",
    "\n",
    "    transition_counts = pl.concat([n_1_transition_counts, n_2_transition_counts, n_3_transition_counts])\n",
    "    \n",
    "    transition_probs = (\n",
    "        transition_counts\n",
    "        .with_columns(pl.col(\"transitions\") / pl.col(\"transitions\").sum().over(\"prev_product_id\"))\n",
    "        .rename({\n",
    "            \"transitions\": \"probability\",\n",
    "            \"product_id\": \"next_product_id\",\n",
    "            \"prev_product_id\": \"product_id\",\n",
    "            })\n",
    "    )\n",
    "    \n",
    "    return transition_probs\n",
    "\n",
    "def predict_based_on_markov_chain(y, transition_matrix, k=50):\n",
    "    \"\"\"\n",
    "    Predicts the next k items based on the transition matrix\n",
    "    \"\"\"\n",
    "    y_pred =  (\n",
    "        y\n",
    "        .drop(\"idx\")\n",
    "        .join(transition_matrix, on=\"product_id\", how=\"left\")\n",
    "        .group_by(\"user_id\")\n",
    "        .agg(\n",
    "            [\n",
    "                pl.struct([\"next_product_id\", \"probability\"])\n",
    "                .sort_by(\"probability\", descending=True)\n",
    "                .slice(0, k)\n",
    "                .alias(\"top_predictions\")\n",
    "            ]\n",
    "        )\n",
    "        .explode(\"top_predictions\")\n",
    "        .select(\n",
    "            \"user_id\",\n",
    "            pl.col(\"top_predictions\").struct.field(\"next_product_id\").alias(\"product_id\"),\n",
    "            pl.col(\"top_predictions\").struct.field(\"probability\").alias(\"probability\"),\n",
    "        )\n",
    "    )\n",
    "    return y_pred\n",
    "\n",
    "def markov_chain_experiment(k_vals = k_vals):\n",
    "    X_train, y_valid, y_test = get_clean_train_valid_eval()\n",
    "    X_test = join_x_y(X_train, y_valid)\n",
    "\n",
    "    transition_matrix = get_3rd_order_markov_chain(X_test)\n",
    "    hits_at_k = []\n",
    "    precision_at_k = []\n",
    "\n",
    "    for k in k_vals:\n",
    "        y_pred = predict_based_on_markov_chain(y_valid, transition_matrix, k = k)\n",
    "        hits = len(y_pred.join(y_test, on=[\"user_id\", \"product_id\"]).collect())\n",
    "\n",
    "        num_predictions = len(y_pred.collect())\n",
    "        number_of_users = len(y_test.collect()) \n",
    "\n",
    "        hits_at_k.append(hits / number_of_users)\n",
    "        precision_at_k.append(hits / k / number_of_users)\n",
    "\n",
    "    results = build_results_df(\"Markov Chain\", k_vals=k_vals, hits_at_k=hits_at_k, precision_at_k=precision_at_k)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37488d-0f2a-40ca-8cff-8550e1f0c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391e614-4aba-4685-99ac-7100294fb221",
   "metadata": {},
   "source": [
    "## Transformer Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c0e2a-45fc-4ac6-962e-7da7625f14d8",
   "metadata": {},
   "source": [
    "### Experiment 3: Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b339e69-fb8e-4776-9889-c5043736c7c9",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbfe155f-3d9d-473d-8387-b2aecbc59ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(block_size=block_size):\n",
    "    X_train, y_train, y_test = get_clean_train_valid_eval()\n",
    "    train = join_x_y(X_train, y_train) \n",
    "    test = join_x_y(train, y_test)\n",
    "    column_ordering = X_train.collect_schema().names()\n",
    "    \n",
    "    assert len(train.collect()) + len(y_test.collect()) == total_len\n",
    "    assert len(test.collect()) == total_len\n",
    "    \n",
    "    train = train.group_by(\"user_id\").tail(block_size).select(column_ordering)\n",
    "    test = test.group_by(\"user_id\").tail(block_size).select(column_ordering)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "class SequentialRecommenderDataset(Dataset):\n",
    "    def __init__(self, data, block_size=block_size):\n",
    "        self.block_size = block_size\n",
    "        self.num_users = data.group_by(\"user_id\").len().collect().shape[0]\n",
    "\n",
    "        samples = (\n",
    "            data\n",
    "            .group_by(\"user_id\")\n",
    "            .agg(pl.col(\"product_id\"))\n",
    "            .collect()\n",
    "            .to_dict()\n",
    "        )\n",
    "        self.samples = {}\n",
    "        self.user_ids = []\n",
    "        \n",
    "        for i, (u_id, p_ids) in enumerate(zip(samples[\"user_id\"], samples[\"product_id\"])):\n",
    "            self.samples[int(u_id)] = torch.tensor(p_ids, dtype=torch.long)\n",
    "            self.user_ids.append(int(u_id))\n",
    "            \n",
    "        assert len(self.user_ids) == self.num_users\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns the sequence of items and sequence of items shifted by 1 to predict\n",
    "        item_sequence = self.samples[self.user_ids[idx]]\n",
    "        x = item_sequence[:-1]\n",
    "        y = item_sequence[1:]\n",
    "        return x, y\n",
    "\n",
    "def padding_collate(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    last_items_idx = torch.tensor([len(seq) - 1 for seq in sequences], dtype=int)\n",
    "    \n",
    "    # THIS IS TERRIBLE, THIS WILL LIKELY CAUSE AN ERORR LATER WHEN I FORGET ABOUT SCOPE\n",
    "    max_len = block_size\n",
    "\n",
    "    padded_sequences = torch.stack([\n",
    "        torch.nn.functional.pad(seq, (0, max_len - seq.size(0)), \"constant\", padding_idx)\n",
    "        for seq in sequences\n",
    "    ])\n",
    "\n",
    "    padded_labels = torch.stack([\n",
    "        torch.nn.functional.pad(label, (0, max_len - label.size(0)), \"constant\", ignore_idx)\n",
    "        for label in labels\n",
    "    ])\n",
    "\n",
    "    # # Find pred indices \n",
    "    # mask = (padded_labels != ignore_idx)  # (B, T)\n",
    "\n",
    "    # pred_idx = mask.long().cumsum(dim=1) - 1\n",
    "    # pred_idx = pred_idx.gather(1, mask.sum(dim=1, keepdim=True) - 1).squeeze(1)\n",
    "    # pred_idx[mask.sum(dim=1) == 0] = -1\n",
    "\n",
    "    return padded_sequences, padded_labels, last_items_idx\n",
    "\n",
    "def get_transformer_datasets():\n",
    "    train, test = get_data()\n",
    "    train_dataset = SequentialRecommenderDataset(train)\n",
    "    test_dataset = SequentialRecommenderDataset(test)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c48a7-b6b2-4d88-be23-e4379a5946dc",
   "metadata": {},
   "source": [
    "#### Transformer Classes\n",
    "Transformer classes based off work from Matthew O'Malley-Nichols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0587327c-77f7-453c-a5ae-42425a8d1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBase(nn.Module):\n",
    "    def __init__(self, embed_dim, block_size, head_dim, autoregression, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim \n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.autoregression = autoregression\n",
    "\n",
    "        # Channels x Head size\n",
    "        self.query_linear = nn.Linear(self.embed_dim, self.head_dim, bias=False)\n",
    "        self.key_linear = nn.Linear(self.embed_dim, self.head_dim, bias=False)\n",
    "        self.value_linear = nn.Linear(self.embed_dim, self.head_dim, bias=False)\n",
    "\n",
    "        if self.autoregression:\n",
    "            self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def get_shape(self, x):\n",
    "        B, T, C = x.shape\n",
    "        assert C == self.embed_dim\n",
    "        return B, T, C\n",
    "\n",
    "    def project_linear_components(self, x):\n",
    "        # Get the linearly projected queries, keys, and values\n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "        return query, key, value\n",
    "\n",
    "    def mask_causal_logits(self, logits, T):\n",
    "        # Mask out future tokens if decoder\n",
    "        if self.autoregression:\n",
    "            logits = logits.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        return logits\n",
    "\n",
    "    def compute_attention(self, logits, value):\n",
    "        weights = F.softmax(logits, dim=-1)\n",
    "        regularized_weights = self.dropout(weights)\n",
    "        attention = torch.einsum(\"btt,btc->btc\", regularized_weights, value)\n",
    "        return attention, weights\n",
    "\n",
    "\n",
    "class SelfAttentionHead(SelfAttentionBase):\n",
    "    # Single head of attention based off \"Let's build GPT: from scratch, in code, spelled out\" by Andrej Karpathy\n",
    "    # and the \"Attention is All You Need\" paper\n",
    "    def __init__(self, embed_dim, block_size, head_dim, autoregression, dropout):\n",
    "        super().__init__(embed_dim=embed_dim, \n",
    "                         block_size=block_size, \n",
    "                         head_dim=head_dim, \n",
    "                         autoregression=autoregression, \n",
    "                         dropout=dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Forward pass for basic self attention\n",
    "        B, T, C = self.get_shape(x)\n",
    "        q, k, v = self.project_linear_components(x)\n",
    "\n",
    "        # Compute attention weights\n",
    "        logits = torch.einsum(\"btc,bTc->bTt\", q, k) # B x T x C @ B x T x C -> B x T x T\n",
    "        logits = logits / (C ** 0.5) # Divide by sqrt(d_k) to prevent peaky softmax\n",
    "\n",
    "        logits = logits.masked_fill(attention_mask, float(\"-inf\"))\n",
    "        logits = self.mask_causal_logits(logits, T)\n",
    "        attention, weights = self.compute_attention(logits, v)\n",
    "\n",
    "        return attention, weights \n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Multiple heads of attention based off \"Let's build GPT: from scratch, in code, spelled out\" by Andrej Karpathy\n",
    "    # and the \"Attention is All You Need\" paper\n",
    "    def __init__(self, num_heads, embed_dim, block_size, autoregression, attention, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self._init_heads(num_heads, embed_dim, block_size, autoregression, attention, dropout)\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _init_heads(self, num_heads, embed_dim, block_size, autoregression, attention, dropout):\n",
    "        head_dim = embed_dim // num_heads\n",
    "        params = {\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"block_size\": block_size,\n",
    "            \"head_dim\": head_dim,\n",
    "            \"autoregression\": autoregression,\n",
    "            \"dropout\": dropout\n",
    "            }\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(**params) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Multihead attention based off \"Attention is All You Need\" paper\n",
    "        attention_maps = []\n",
    "        attentions = []\n",
    "        for head in self.heads:\n",
    "            attention_output, attention_map = head(x, attention_mask)\n",
    "            attention_maps.append(attention_map)\n",
    "            attentions.append(attention_output)\n",
    "\n",
    "        concatenated_attention = torch.cat(attentions, dim=-1)\n",
    "        concatenated_attention = self.dropout(self.proj(concatenated_attention))\n",
    "        return concatenated_attention, attention_maps\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # Feed forward network based off \"Let's build GPT: from scratch, in code, spelled out\" by Andrej Karpathy\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff_net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff_net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    # Single transformer block based off \"Attention is All You Need\" paper\n",
    "    def __init__(self, embed_dim, num_heads, block_size, hidden_dim, autoregression, attention, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, \n",
    "                                            embed_dim=embed_dim, \n",
    "                                            block_size=block_size, \n",
    "                                            autoregression=autoregression, \n",
    "                                            attention=attention, \n",
    "                                            dropout=dropout)\n",
    "\n",
    "        self.feed_forward = FeedForward(embed_dim=embed_dim, \n",
    "                                        hidden_dim=hidden_dim, \n",
    "                                        dropout=dropout)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim) # Pre-normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Residual connection around each sub-block\n",
    "        attentions, attention_maps = self.attention(self.layer_norm1(x), attention_mask)\n",
    "        with torch.no_grad(): # Discard gradients for the attention maps\n",
    "            attention_maps = [attn_map.detach() for attn_map in attention_maps]\n",
    "        x = x + attentions\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x, attention_maps\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, block_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Embedding(block_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        return self.positional_embedding(torch.arange(T, device=x.device))\n",
    "\n",
    "class TransformerBase(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, num_heads, num_layers, hidden_dim, autoregression, attention=\"basic\", position_encoding=\"learned\", dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.position_encoding = LearnedPositionalEncoding(block_size, embed_dim)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size+1, embed_dim, padding_idx=0)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, block_size, hidden_dim, autoregression, attention, dropout) \n",
    "            for _ in range(num_layers)\n",
    "            ])\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            # nn.init.kaiming_normal_(module.weight)\n",
    "            # Normal distribution reduces train perplexity to be within range, at cost of test perplexity\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "\n",
    "    def embed(self, x):\n",
    "        # Return the embeddings for the input sequence\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.token_embedding_table(x)\n",
    "        if not self.position_encoding:\n",
    "            return token_embeddings\n",
    "        pos_embeddings = self.position_encoding(x)\n",
    "        return token_embeddings + pos_embeddings\n",
    "\n",
    "class Decoder(TransformerBase):\n",
    "    def __init__(self, vocab_size=num_items, embed_dim=embed_dim, block_size=block_size, \n",
    "                 num_heads=2, num_layers=4, hidden_dim=embed_dim*4, \n",
    "                 attention=\"basic\", position_encoding=\"learned\", dropout=0.0, neg_ce_loss=False):\n",
    "        super().__init__(vocab_size=vocab_size,\n",
    "                         embed_dim=embed_dim,\n",
    "                         block_size=block_size,\n",
    "                         num_heads=num_heads,\n",
    "                         num_layers=num_layers,\n",
    "                         hidden_dim=hidden_dim,\n",
    "                         autoregression=True,\n",
    "                         position_encoding=position_encoding,\n",
    "                         attention=attention,\n",
    "                         dropout=dropout)\n",
    "        self.neg_ce_loss = neg_ce_loss\n",
    "        self.rec_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        attention_mask = x == 0 # B x T  \n",
    "        attention_mask = attention_mask.unsqueeze(1)  # Shape: (B, 1, T)\n",
    "        attention_mask = attention_mask.expand(-1, self.block_size, -1)  # Shape: (B, T, T)\n",
    "\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        attention_maps = []\n",
    "        for block in self.blocks:\n",
    "            x, maps = block(x, attention_mask)\n",
    "            attention_maps.extend(maps)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.rec_head(x) # B x T x V\n",
    "\n",
    "        cross_entropy_loss = None\n",
    "        if y is not None:\n",
    "            logits_flat = logits.view(-1, logits.size(-1))\n",
    "            y_flat = y.view(-1)\n",
    "            cross_entropy_loss = F.cross_entropy(logits_flat, y_flat, ignore_index=ignore_idx)\n",
    "            if self.neg_ce_loss:\n",
    "                with torch.no_grad():\n",
    "                    B = x.size(0)\n",
    "                    for i in range(B):\n",
    "                        neg_mask = torch.arange(B) != i\n",
    "                        neg_logits = logits[neg_mask]\n",
    "                        neg_loss = -torch.mean(torch.log_softmax(neg_logits.view(-1, neg_logits.size(-1)), dim=-1).sum(dim=-1) + 1e-12)\n",
    "                        cross_entropy_loss += neg_loss\n",
    "\n",
    "        return logits, cross_entropy_loss, attention_maps\n",
    "\n",
    "    def get_batch_metrics_at_k(self, logits, Y, last_items_idx, k):\n",
    "        with torch.no_grad():\n",
    "            row_indices = torch.arange(batch_size)[:len(Y)]\n",
    "            last_targets = Y[row_indices, last_items_idx] \n",
    "            \n",
    "            pred = logits[row_indices, last_items_idx, :]\n",
    "\n",
    "            prob = F.softmax(pred, dim=-1)\n",
    "            \n",
    "            top_k_values, top_k_indices = torch.topk(prob, k=k, dim=-1)\n",
    "            \n",
    "            hits = (last_targets.unsqueeze(1) == top_k_indices)  # Shape: [B, k]\n",
    "            hit_count = hits.any(dim=1).sum().item()\n",
    "            hit_at_k = hit_count / batch_size\n",
    "\n",
    "            dcg = (hits.float() / torch.log2(torch.arange(2, k + 2, device=logits.device).float())).sum(dim=1)\n",
    "            ideal_relevance = torch.ones(len(row_indices), k, device=logits.device)\n",
    "            idcg = (ideal_relevance / torch.log2(torch.arange(2, k + 2, device=logits.device).float())).sum(dim=1)\n",
    "            \n",
    "            ndcg = (dcg / idcg)\n",
    "            ndcg[torch.isnan(ndcg)] = 0.0\n",
    "            ndcg = ndcg.mean().item()\n",
    "        \n",
    "            return hit_at_k, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c06c1-d505-42a7-a42c-d9b79ed4e9fa",
   "metadata": {},
   "source": [
    "#### Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a96f4fc7-b1f1-4b70-9db9-1442400e8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(num_epochs=50, batch_size=32, num_items=num_items, learning_rate=learning_rate, k_vals=k_vals, model_name=\"decoder_ce\"):\n",
    "    # Overfit quickly with num_heads =2 and num_layers =4 w/ 1e-3 learning rate\n",
    "    train_dataset, test_dataset = get_transformer_datasets()\n",
    "    del test_dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=padding_collate)\n",
    "    #test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=padding_collate)\n",
    "    model = Decoder(dropout=dropout, neg_ce_loss=True)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(device)\n",
    "    \n",
    "    model.train()\n",
    "    max_hit = -np.inf\n",
    "    patience = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        total_hits = 0\n",
    "        print(epoch)\n",
    "        for batch, (X, Y, last_items_idx) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)  \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss, _ = model(X, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                pass\n",
    "                #last_items_idx.to(device)\n",
    "                #hit, ncdg = model.get_batch_metrics_at_k(logits, Y, last_items_idx, 50)\n",
    "                #total_hits += hit\n",
    "                # print(f\"Epoch {epoch}, Batch {batch}, Loss {loss}\")\n",
    "                # print(f\"Epoch {epoch}, Batch {batch}, hit@k {hit, ncdg}\")\n",
    "        if total_hits < max_hit:\n",
    "            patience -= 1 \n",
    "        else:\n",
    "            min_hit = total_hits\n",
    "        if patience <= 0:\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), f\"./models/{model_name}_{epoch}.pth\")\n",
    "\n",
    "train_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6609f-aaea-48c6-831e-8e4c74672849",
   "metadata": {},
   "source": [
    "#### Transformer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff247ad8-3629-4073-b5c3-163504c8a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3961/1665259353.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(state_dict_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>k</th><th>hits_at_k</th><th>ncdg_at_k</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;decoder_bce_50_5&quot;</td><td>10</td><td>0.07615</td><td>0.008624</td></tr><tr><td>&quot;decoder_bce_50_5&quot;</td><td>25</td><td>0.14069</td><td>0.00674</td></tr><tr><td>&quot;decoder_bce_50_5&quot;</td><td>50</td><td>0.21326</td><td>0.005329</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌──────────────────┬─────┬───────────┬───────────┐\n",
       "│ model            ┆ k   ┆ hits_at_k ┆ ncdg_at_k │\n",
       "│ ---              ┆ --- ┆ ---       ┆ ---       │\n",
       "│ str              ┆ i64 ┆ f64       ┆ f64       │\n",
       "╞══════════════════╪═════╪═══════════╪═══════════╡\n",
       "│ decoder_bce_50_5 ┆ 10  ┆ 0.07615   ┆ 0.008624  │\n",
       "│ decoder_bce_50_5 ┆ 25  ┆ 0.14069   ┆ 0.00674   │\n",
       "│ decoder_bce_50_5 ┆ 50  ┆ 0.21326   ┆ 0.005329  │\n",
       "└──────────────────┴─────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_eval(model_str=\"decoder_bce_50_5\", k_vals=k_vals, batch_size=batch_size, num_items=num_items):\n",
    "    train_dataset, test_dataset = get_transformer_datasets()\n",
    "    del train_dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=padding_collate)\n",
    "\n",
    "    state_dict_path = f\"./models/{model_str}.pth\"\n",
    "    model = Decoder(dropout=dropout)\n",
    "    model.load_state_dict(torch.load(state_dict_path))\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    results = {}\n",
    "\n",
    "    hits_at_k = [0.0 for k in k_vals]\n",
    "    ncdg_at_k = [0.0 for k in k_vals]\n",
    "\n",
    "    num_batches = 0 \n",
    "    for batch, (X, Y, last_items_idx) in enumerate(test_loader):\n",
    "        num_batches += 1\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)  \n",
    "        last_items_idx = last_items_idx.to(device)\n",
    "        logits, _, _ = model(X)\n",
    "\n",
    "        for i, k in enumerate(k_vals):\n",
    "            hit, ncdg = model.get_batch_metrics_at_k(logits, Y, last_items_idx, k)\n",
    "            hits_at_k[i] += hit\n",
    "            ncdg_at_k[i] += ncdg\n",
    "\n",
    "    ncdg_at_k = np.array(ncdg_at_k) / num_batches \n",
    "    hits_at_k = np.array(hits_at_k) / num_batches \n",
    "\n",
    "    return pl.DataFrame(\n",
    "        {\n",
    "            \"model\": [f\"{model_str}\"] * len(k_vals),\n",
    "            \"k\": k_vals,\n",
    "            \"hits_at_k\": hits_at_k,\n",
    "            \"ncdg_at_k\": ncdg_at_k\n",
    "        }\n",
    "    )\n",
    "transformer_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6662f7ad-43e5-443c-a99a-6b31bb5770da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_results():\n",
    "    bert4rec = pl.read_csv(\"./results/bert4rec.csv\").drop(\"precision_at_k\")\n",
    "    decoder_ce_15 = pl.read_csv(\"./results/decoder_ce_15.csv\").drop(\"ncdg_at_k\").with_columns(pl.col(\"model\").replace(\"decoder_ce_50_15\", \"D-CE\"))\n",
    "    decoder_bce_15 = pl.read_csv(\"./results/decoder_bce_15.csv\").drop(\"ncdg_at_k\").with_columns(pl.col(\"model\").replace(\"decoder_bce_50_15\", \"D-CCE\"))\n",
    "    popularity = pl.read_csv(\"./results/popularity_baseline.csv\").drop(\"precision_at_k\").with_columns(pl.col(\"model\").replace(\"popularity\", \"PopRec\"))\n",
    "    markov_chain = pl.read_csv(\"./results/markov_chain.csv\").drop(\"precision_at_k\")\n",
    "    results = pl.concat([bert4rec, decoder_ce_15, decoder_bce_15, popularity, markov_chain]).filter(pl.col(\"k\") > 5)\n",
    "    # Round all to 4 decimal places \n",
    "    results = (\n",
    "        results\n",
    "        .with_columns(\n",
    "            pl.col(\"hits_at_k\").round(4),\n",
    "        )\n",
    "        .pivot(on=\"k\", index=\"model\", values=\"hits_at_k\")\n",
    "        .sort(\"50\", descending=True)\n",
    "    )\n",
    "    return results\n",
    "\n",
    "concat_results().write_csv(\"./results/total_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6b9f2ab-df31-4cce-8d83-5b6b595df73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyperparameters():\n",
    "    hp = pl.DataFrame(\n",
    "        {\n",
    "            \"Hyperparameter\": [\"Batch Size\",\n",
    "                               \"Block Size\", \n",
    "                               \"Dropout\",\n",
    "                               \"Embedding Dim\", \n",
    "                               \"Epochs\",\n",
    "                               \"Hidden Dim\", \n",
    "                               \"Learning Rate\", ],\n",
    "            \"Bert4Rec\": [256, 50, 0.5, 64, 5, 64, 0.001], \n",
    "            \"D-CE\": [128, 64, 0.5, 64, 15, 256, 0.001],\n",
    "            \"D-CCE\": [32, 64, 0.5, 64, 15, 256, 0.001],\n",
    "        }, strict=False\n",
    "    )\n",
    "    return hp\n",
    "print_hyperparameters().write_csv(\"./results/hyperparameters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58751172-3566-4f13-a5e6-9ba309f82cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
